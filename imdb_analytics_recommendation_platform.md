
# ğŸ¬ IMDb Analytics & Recommendation Platform

## ğŸ“Œ Project Overview
The **IMDb Analytics & Recommendation Platform** is an end-to-end data engineering and analytics project built using **AWS and Python**. The goal of this project is to ingest large-scale IMDb public datasets, store them in a cloud-based data lake, and prepare them for analytics and recommendation use cases.

This repository currently focuses on the **data ingestion pipeline**, which uploads raw IMDb CSV datasets from a local environment to **AWS S3** using **boto3**, following industry-standard data lake design principles.

---

## ğŸ—ï¸ Architecture

```
Local Machine (IMDb CSV files)
        â†“
Python + boto3
        â†“
AWS S3 (Raw Data Layer)
        â†“
Snowflake (next phase)
        â†“
Analytics & Recommendation Engine
```

---

## ğŸ“‚ Dataset Description
The project uses the official IMDb public datasets:

| Dataset | Description |
|-------|------------|
| name.basics.csv | Actor, director, and crew details |
| title.basics.csv | Core movie/TV metadata |
| title.ratings.csv | IMDb ratings and vote counts |
| title.crew.csv | Directors and writers |
| title.principals.csv | Cast and crew roles |
| title.episode.csv | TV episode information |
| title.akas.csv | Regional and alternate titles |

---

## ğŸ—‚ï¸ S3 Data Lake Structure

```
s3://imdb-analytics-data-2025/
 â””â”€â”€ raw/
     â”œâ”€â”€ name.basics.csv
     â”œâ”€â”€ title.basics.csv
     â”œâ”€â”€ title.ratings.csv
     â”œâ”€â”€ title.crew.csv
     â”œâ”€â”€ title.principals.csv
     â”œâ”€â”€ title.episode.csv
     â””â”€â”€ title.akas.csv
```

- **Raw Layer**: Stores unmodified source data
- Enables reproducibility and reprocessing

---

## âš™ï¸ Tech Stack

- **Language:** Python 3.11
- **Cloud:** AWS S3
- **SDK:** boto3
- **CLI:** AWS CLI v2
- **Version Control:** Git & GitHub

---

## ğŸš€ Pipeline: Upload IMDb Datasets to S3

### Key Features
- Programmatic S3 bucket creation
- Bulk CSV uploads from local folder
- Structured S3 folder (`raw/`)
- Error handling and logging
- IAM-based authentication (no hardcoded keys)

### Script

```bash
upload_imdb_datasets_to_s3.py
```

The script:
1. Checks if the S3 bucket exists
2. Creates the bucket if missing
3. Uploads all IMDb CSV files from a local directory
4. Stores them in the S3 `raw/` layer

---

## â–¶ï¸ How to Run

### 1. Configure AWS Credentials

```bash
aws configure
```

### 2. Install Dependencies

```bash
pip install boto3
```

### 3. Run the Script

```bash
python upload_imdb_datasets_to_s3.py
```

---

## âœ… Sample Output

```
Bucket 'imdb-analytics-data-2025' created successfully!
Uploaded: title.basics.csv
Uploaded: title.ratings.csv
...
All files uploaded successfully
```

---

## ğŸ” Security Best Practices

- IAM user (not root)
- No credentials committed to GitHub
- `.gitignore` used to exclude sensitive files

---
## Project Structure

src/   â†’ Python ingestion scripts  
sql/   â†’ Snowflake SQL (staging, dimensions, facts)  
docs/  â†’ Architecture and notes  

## ğŸ“ˆ Future Enhancements

- Snowflake external stage from S3
- Staging, fact, and dimension tables
- Automated ingestion using Snowflake Tasks / Airflow
- Data quality checks
- Analytics dashboards (Power BI / Tableau)
- Content-based recommendation engine


## ğŸ‘¤ Author
**Manichandra Domala**

---

â­ If you find this project useful, feel free to star the repository!

